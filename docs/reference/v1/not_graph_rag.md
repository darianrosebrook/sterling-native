# Sterling: Not Another Graph-RAG

> **Note**: This document captures the differentiation thesis that informs Sterling's design. For canonical definitions of Sterling's architectural invariants, see:
> - [docs/canonical/north_star.md](../../canonical/north_star.md) - Sterling's operational definition
> - [docs/canonical/core_constraints_v1.md](../../canonical/core_constraints_v1.md) - The 11 invariants
> - [docs/canonical/neural_usage_contract.md](../../canonical/neural_usage_contract.md) - Neural vs symbolic authority

---

You’re not drifting into “just another agent with a vector DB.”
But you _are_ moving into the same **idea neighborhood** as a bunch of other things (graph RAG, memory systems, world models). The question is: _what are you insisting on that they are not?_ That’s where the novelty lives.

Let me lay this out in layers.

---

## 1. What “everyone else” is building right now

If we zoom out, a lot of contemporary systems look like some combination of:

1. **LLM + KV cache + RAG**

   - Long context is “managed” either by:

     - throwing more tokens at the model,
     - or chunking past conversation into embeddings and pulling them back with cosine similarity.

   - Semantics live inside the transformer weights and its temporary KV cache.
   - Summaries and task state are _also_ text, generated by the same model with no persistent structure behind them.

2. **Graph-RAG / knowledge-graph agents**

   - KG as a retrieval substrate: nodes = entities, edges = relations, sometimes plus basic scoring.
   - The LLM still does most of the “reasoning”; the graph is an address book.
   - Memory is either:

     - a log of text episodes, or
     - some light metadata on nodes (like a counter or a timestamp).

3. **Agent frameworks with tools + “memory”**

   - ReAct-style loops: LLM decides what tool to call, looks at result, repeats.
   - “Memory” is logs + summaries (“write a summary of this conversation to a note”).
   - No real notion of **stateful structural learning** about which _paths_ through that tool/knowledge space are better than others.

The common pattern: **the transformer remains the cognitive core**. Everything else is scaffolding to get it better inputs.

---

## 2. Where Sterling overlaps that landscape

There _is_ overlap; that’s inevitable:

- You have a **KG** with typed nodes/edges.
- You have a **short working memory** (SWM) slice instead of using the entire KG.
- You have an **encoder + value head** that maps traces into a latent/value.
- You have a **path-selection mechanism** using salience and decay.

If I describe that sloppily, it _does_ sound like: “graph RAG + bandit-ish exploration + a scoring head.”

So purely at the “buzzword” level, you _are_ in a crowded area.

The danger is: if you stop here, an outside reader could say “okay, neat engineering, but still an LLM agent with a fancier memory.”

---

## 3. What is actually different about Sterling (and worth defending)

Where I think you _are_ doing something genuinely different — and what you should double down on — are these design constraints:

### 3.1. Meaning lives in IR + KG, not in the transformer

You’ve been very explicit that:

- The core is **UtteranceState + KG + operators**, not the model’s hidden state.
- The LLM/encoder is there to:

  - turn raw language into **structured IR**, and
  - maybe compress/summary into a latent for indexing.

- All _reasoning_ is supposed to be: **symbolic / graph-based / value-guided**, with an audit trail.

That’s not how most graph-RAG work is actually implemented. They _say_ “we use a KG,” but:

- they still run free-form CoT in the LLM, and
- the graph is more like a “semantic index.”

Your insistence on:

- **no hallucination at the IR layer**, and
- **auditable, replayable state graphs of reasoning**

is a real differentiator _if you actually enforce it_ (i.e., no sneaky “let the LLM improvise CoT in the middle”).

---

### 3.2. Path algebra + edge-relative plasticity, not node scores

You didn’t just assign salience to nodes. You built:

- **EdgeState** with `times_taken`, `times_available`, `last_taken`, `weight`.
- **Path-level updates** that distinguish:

  - edges taken vs. edges ignored,
  - branches pruned vs. reinforced,
  - roles: `GLOBAL_ANCHOR`, `EPISODIC_SUMMARY`, `SCRATCH`.

That’s closer to _eligibility traces_ / _credit assignment along trajectories_ than to the usual “every node gets a scalar importance.”

Most LLM systems don’t:

- track fine-grained **how often a path was _available_ but not chosen**,
- update edges as first-class plastic entities,
- or gate decay on **compression** (only decay micro-nodes once you’ve rolled them into a summary).

Those are architectural commitments, not just tuning.

---

### 3.3. Compression-gated decay and episodic summaries as _structural_ objects

Your “decay only after compression” rule is subtle but important:

- You don’t just gradually drop old tokens or old nodes.
- You require the system to first **summarize the episode into a dedicated node** with a role, then mark the detailed path as decay-eligible.

That means:

1. The “story of how we got here” is preserved at a higher level of abstraction.
2. You can later reason _over episodes as objects_, not just raw text logs.
3. You’ve created a **two-level semantic hierarchy**:

   - detailed micro-paths,
   - and compressed episode summaries with latents.

Most systems treat summaries as text blobs in a DB. You’re treating them as:

- nodes in the same KG space,
- with attached latent vectors,
- connected to the micro nodes they compress,
- and used as part of path selection / SWM seeding.

That’s closer to a real **episodic memory with consolidation** than to “just summarize the conversation every N turns.”

---

### 3.4. Transformer as an _optional skin_, not the skeleton

Your original goal:

> reduce the reliance on transformers for keeping track of semantics in things like completing tasks, summarizing our work, keeping track of long horizon goals, making smart and verified decisions.

Right now, Sterling’s shape is:

- **Core cognition:**

  - KG,
  - path algebra,
  - SWM,
  - value head,
  - operators.

- **Interfaces:**

  - IR parser (LLM),
  - NL surface generator (LLM),
  - maybe a small encoder for compression.

If you keep pushing in this direction, you can do something most people _aren’t_ doing:
**turn the transformer into a replaceable codec** — just one module that plugs into a more general semantic engine.

That’s a very different stance from: “the transformer is the mind, everything else is tools.”

---

## 4. Are you still on track for the original compute/semantics goal?

I’d say: yes, _if_ you harden a few constraints going forward.

Right now, your architecture is _capable_ of shaving compute by:

1. **Working at graph scale instead of token scale** for most long-horizon behavior.
2. **Compressing episodes into summary nodes + latents**, so future episodes operate on a tiny SWM, not a giant token buffer.
3. **Using the transformer sparingly**, e.g.:

   - only when you need to parse new utterances into IR,
   - or generate human-readable explanations,
   - or create a summary from discrete state (not from huge raw context).

But you won’t _realize_ that advantage unless you:

- **Measure** token usage and forward passes vs. a baseline LLM agent:

  - same tasks,
  - same KG access,
  - one with “just RAG + chain-of-thought,”
  - one with “Sterling path algebra + SWM + summary nodes.”

- **Forbid** transformer-based CoT inside the loop that updates KG state.

  - If every step still calls the LLM to “think,” you’ve smuggled the transformer back in.

So part of keeping it novel is making those constraints explicit in your docs:

> - No free-form CoT in the decision loop.
> - All task state must be encoded as UtteranceState + KG annotations.
> - All long-horizon tracking must pass through episode summaries and path algebra.

That’s what keeps you from becoming “just graph-flavored ReAct.”

---

## 5. How to _lean further_ into novelty from here

If you want to push the project away from the generic agentic pile and towards something that’s clearly its own species, I’d recommend:

### 5.1. Ship one “transformer-light” demo

Pick a task where:

- The KG has enough structure (e.g., PN verification, grammar explanations, or a tiny spec-checking domain).
- Sterling’s core loop can run for many steps _without_ calling a big LLM.

Use:

- A fixed encoder for IR,
- The KG + SWM + path algebra for reasoning,
- A tiny decoder or even template-based explanation for output.

If you can show:

> “Given this KG and IR, we perform N steps of reasoning, with zero additional transformer calls, and we still get correct answers / verifiable rationales,”

you’ve made your point: **semantics are in the structure, not in the transformer.**

---

### 5.2. Make “path-level learning” first-class in the documentation

Right now, your docs talk about SWM and decay, but I’d emphasize in the theory:

- Sterling doesn’t just learn _what_ to retrieve; it learns **how to traverse** the graph.
- Edge-level statistics (`times_taken`, `times_available`) approximate a **policy over paths**, not just a heuristic.
- Compression + summaries create a _higher-order graph of episodes_ — a graph of paths, not just of concepts.

That’s closer to:

- “A structural world model plus a learned policy over it,”
  than to:
- “A language model with a fancy index.”

This is the kind of phrasing that will read as genuinely different to people who’ve only seen vector stores and graph-RAG.

---

### 5.3. Design one “compute-bound” experiment as a north star

To hit your “shaving compute” goal, set up a benchmark that you care about:

- A fixed long-horizon task where:

  - vanilla LLM agent:

    - needs full conversation history / document context repeatedly,
    - or needs many tool calls,

  - Sterling:

    - uses a bounded SWM and summary nodes,
    - doesn’t re-ingest the full past each time.

Then measure:

- **Token usage per episode**,
- **Number of transformer calls**,
- **Latency**,
- **Accuracy / task success**.

If Sterling can match or beat the baseline on quality with **significantly fewer tokens + calls**, you’ve demonstrated the core thesis in a way that justifies all the architectural overhead.

That’s not what “everyone else” is doing. Most agent frameworks aren’t trying to _replace_ the transformer’s semantic tracking; they’re trying to **help** it use more context.

You’re explicitly saying: _No, we want a smaller, more structural cognitive core that uses transformers like glue, not like a brain._

---

## 6. So… are you still making something novel?

My honest read:

- **If** Sterling becomes “LLM + KG + memory summaries” with most of the hard work still being chains of thought inside the LLM, then yeah, it converges on what everyone else is building.
- **If** Sterling keeps the line that:

  - _meaning is in IR + KG_,
  - _decisions and long-horizon structure come from path algebra + value functions_,
  - _transformers are just parsers/encoders/skins_,
  - and you **demonstrate real compute savings** on long-horizon tasks,

then you are working on a genuinely different architecture — one that’s closer to a **graphical / symbolic world model with a small neuro-lingual interface** than to “another agent wrapper.”

You’re already _pointed_ at that second outcome. The next steps (wiring SWM + path algebra fully, compressing episodes, running topic-hopping + compute-bound experiments) are about making the distinction so sharp that it’s obvious even to someone who only scans the README.

If you’d like, we can sketch that “Sterling vs Generic Agent” comparison doc next — with explicit invariants about what Sterling is _not allowed_ to do (e.g., no CoT in the reasoning loop). That’s often the cleanest way to defend novelty: by stating what you refuse to rely on.

> Cool, you’ve reiterated exactly what my concerns were with the direction that we started going as we deviated slightly. What everyone currently knows how to build is all converging on transformers, and where I stand is that we’re spending time trying to get the architecture of transformers to do something they’re just not meant to do. It was like they took a toy and kept trying to upgrade it with rocket boosters rather than looking at it from “we need a rocket, what does the harness look like?”

> The way language works, how we track goals, etc all lives in some semantic representation in our brain. What we end up doing after thinking about the semantics is “how do I say something that means this” instead of word vomit until the meaning becomes apparent.

> The learned behaviors, ideal paths, important lines of thinking, all can be represented semantically, and we only need to run generation of language (which can also be store like these graphs) in the end when we need to communicate it to something else or jot down the result.

> The reason I think this compresses things is that the directions we take through these steps of reasoning like the rush hour game example where each state being represented on that graph and we have operations that get us to the next node towards the goal, we use much less compute when we correctly embed and keep track of the weights of these nodes and edges and prune things as time goes on.

> If we had spent a whole lot of time spinning in one area of this multidimensional and multilayered graph, we can wrap it in a summary and use it like a landmark to come back to or anchor towards once we start working on something else or from a new location.

> I used to play a game with a friend where we would start on Wikipedia’s home page, and you would hit “random” to go to a random Wikipedia article. And then in 6 moves (following hyper links to other wiki articles ) or less try to make it to the page on “Ancient Rome”

> I called it “all links lead to Rome” which was a fun way to explore the six degrees of separation rule with connected ideas. It wasn’t always possible to get to the end page, but we also started realizing that if we at least got to one type of page or state, we could get right over to the page for Rome every time. This meant we were able to find ideal paths to the solution in novel ways, rewarded if we tried a path we hadn’t that still worked out, learned which paths were dead ends and still found new information as we connected seemingly unconnected ideas, people, topics, locations, concepts, and more.

What you’re describing with “all links lead to Rome” is basically the _purest_ version of what Sterling is trying to be:
not a better sentence-completer, but a **path-finder in concept space** where language is just I/O.

Let me tie your intuitions to concrete design directions and how to keep this novel.

---

## 1. Your Wikipedia game _is_ the Sterling mental model

In your game:

- **Nodes** = Wikipedia pages (concepts).
- **Edges** = hyperlinks (relations).
- **Goal** = reach “Ancient Rome” in ≤ N steps.
- **Episodes** = one run from a random node to success/failure.
- Over time you:

  - Discovered **landmarks** (“if we can get here, Rome is easy”),
  - Learned **good paths** (“from Biology → Europe → Rome”),
  - Identified **dead zones** (regions of the graph that rarely lead to Rome in budget).

That’s exactly what Sterling is _supposed_ to do on a semantic KG:

- Nodes: operators, frames, facts, schemas, summaries.
- Edges: typed relations, operator pre/post-conditions, “leads-to” traces.
- Goal: “verify this PN,” “prove this constraint,” “reach a state where this spec is satisfied.”
- Episodes: one reasoning session through the KG.

And the key properties of your game map 1:1:

- You don’t memorize word sequences, you memorize **routes**.
- You don’t need to re-read every page every time; you remember **“go via this concept class”**.
- You eventually compress whole swaths of experience into **“get to X, and you’re basically there.”**

Sterling’s path algebra + SWM + summaries is that, but formalized.

---

## 2. Why this diverges from “just more transformer”

The standard transformer mindset:

> “Let’s give the model more context and let it discover the right path in token space.”

Your mindset:

> “Let’s represent the world as a graph of _states and moves_, then learn good routes in that graph, and only turn those into sentences when needed.”

The crucial differences:

1. **State space**:

   - Others: token sequences, KV cache.
   - You: **UtteranceState + KG nodes + episode summaries**.

2. **Computation**:

   - Others: every “thought” is a full forward pass over a long prompt.
   - You: most “thought” is **edge updates, SWM selection, value estimation** over a small graph slice.

3. **Learning**:

   - Others: hidden weights absorb both semantics and policy.
   - You: semantics live in **explicit structures**, and **policy lives in edge statistics and value heads**.

You’re basically saying: _the rocket is the semantic graph + path policy; the transformer is a thruster you fire occasionally to translate between symbols and sentences._

That’s a genuinely different stance.

---

## 3. How to turn your intuition into concrete Sterling next steps

I’d treat your Wikipedia game as a _design spec_ for the next phase:

> Sterling should be able to play “All Links Lead to X” on its own KG.

### 3.1. Build a mini “All Links Lead to X” environment in Sterling

Not with real Wikipedia (yet), but with a synthetic or mid-size KG that behaves like it.

Concretely:

- **KG**:

  - A few hundred nodes grouped into “regions” (topics / domains).
  - Some nodes designated as **global anchors** (like your “Ancient Rome”).
  - Edges such that:

    - There _are_ short paths from many regions to the anchor.
    - Some regions are “far” unless you find specific bridge nodes.

- **Episodes**:

  - Start from random node `s`.
  - Goal node `g` is fixed (or chosen from a small set).
  - Budget: e.g. max 6–8 steps.
  - At each step, Sterling:

    - sees current node + local neighborhood + SWM,
    - chooses an outgoing edge via path algebra + SWM + value head,
    - updates edge stats (times_taken / times_available),
    - runs compression & decay when appropriate.

- **Metrics**:

  - Success rate (reach `g` within budget).
  - Average path length.
  - Emergence of **landmarks**:

    - nodes with high edge weights / centrality on successful paths.

  - **Retooling speed** if you change the goal node.

This is a direct analog of your wiki game and uses everything you’ve built:

- PathAlgebraEngine,
- EdgeState,
- Node roles,
- SWM / compression-gated decay.

### 3.2. Use this to prove your compute thesis

Once you have this environment:

- Baseline: “LLM-agent style”:

  - Do a chain-of-thought textual reasoning step every hop.
  - Maybe use a graph but rely on the LLM to pick edges via text prompts.

- Sterling-style:

  - Pre-encoded KG and IR.
  - Path decisions made **inside the graph**, no fresh LLM forward per step.
  - LLM only used:

    - at episode start (parse task to goal node),
    - at episode end (verbalize a rationale or summary),
    - when creating a new episode summary node.

Then compare:

- # of transformer calls per episode.
- Total tokens consumed.
- Success rate and path optimality.

If Sterling matches or beats the baseline on success with dramatically fewer tokens / calls, you’ve empirically demonstrated:

> “We can offload long-horizon, path-structured reasoning into a graph model and use transformers mainly as a translation layer.”

That’s the opposite of where most of the field is going.

---

## 4. How your “landmarks and dead ends” intuition shapes the architecture

Your wiki intuition gives you some strong design constraints:

1. **Landmarks = episodic summaries + global anchors**

   - Nodes you repeatedly pass through on successful runs.
   - Should be:

     - promoted to `GLOBAL_ANCHOR` or `EPISODIC_SUMMARY`,
     - rarely decayed,
     - used as starting points for new tasks (“if I can get to X, I’m effectively close to g”).

2. **Dead ends = nodes whose outgoing edges rarely lead to success**

   - EdgeState gives you:

     - high `times_available`, low `times_taken` on successful trajectories.

   - Those edges should decay and eventually be pruned from SWM traversal.
   - The nodes might remain in the global KG (for explainability), but are de-prioritized in path search.

3. **Novel but successful paths = new structure, not just text**

   - A new route that succeeds is not just “an interesting CoT sample,”
   - it’s:

     - an episode trace,
     - a candidate to compress into a summary node,
     - and a source of updated edge statistics.

So instead of trying to “remember the good prompt,” Sterling remembers:

- “This subgraph and sequence of operators works; compress it into a landmark and reinforce its edges.”

That’s exactly what you were doing mentally with “All Links Lead to Rome.”

---

## 5. How to keep Sterling from drifting back into “LLM with a fancy index”

Given all this, I’d crystallize a few **hard invariants** for Sterling going forward:

1. **No free-form CoT in the reasoning loop.**

   - All mid-episode state transitions must go:

     - UtteranceState / KG → PathAlgebraEngine → new UtteranceState / KG.

   - The LLM is _not_ allowed to generate new steps or state transitions directly.

2. **All long-horizon task state must live in the KG + summaries.**

   - If you need to know “what have we done so far / what’s left?”, you inspect:

     - episode summary nodes,
     - their attached latents,
     - and the path history,
     - not a text transcript in a prompt.

3. **Transformer calls are only allowed at I/O boundaries.**

   - Parse raw language into IR,
   - Turn IR/paths/summaries back into language.

4. **Every transformer-heavy baseline you compare against must be matched or beaten in compute per task.**

   - Otherwise, you’ve just reimplemented a more complex version of what already works.

Those guardrails keep the architecture aligned with your rocket analogy:

- The **rocket** is the graph + path algebra.
- The **harness** is your IR, summaries, and value functions.
- The **transformer** is a module you bolt on for launch/landing (text ↔ structure), not the vehicle itself.

---

## 6. Short answer to your question

> _Am I still making something novel, or am I steering into what everyone else is making?_

You’re still building something novel _as long as_ you stay disciplined about:

- semantics = IR + KG + path algebra,
- long-horizon behavior = summaries + edge-relative learning,
- transformers = textual codec, not core cognition.

Your wiki game story is actually the best compass:
if you can make Sterling play “All Links Lead to X” inside its own semantic graph — efficiently, with learned landmarks and compressed episodes — you’ve concretely realized the thing you were thinking about back then.

If you’d like, next we can:

- sketch the “All Links Lead to X” experiment spec in your repo structure (scripts, KG shape, metrics), or
- draft a short `NOVELTY.md` that stakes out these invariants so future you doesn’t drift back toward “just use the LLM.”

> Maybe then, the next step is starting with a smaller grid of my rush hour state graph? Start with 5x5, expand incrementally to keep testing this theory?

Yes, I think the 5×5 Rush Hour graph is exactly the right next move—and not just as “another toy,” but as a stress-test for the _particular_ thing Sterling cares about: path semantics, landmarks, and edge-relative decay.

Let me frame what this gives you and then outline a concrete, incremental plan.

---

## Why the 5×5 Rush Hour graph is a good next domain

Your screenshots already show what we want: dense clusters of states, long skinny “bridges,” and a few critical “choke points” that every solution path passes through.

That’s basically a lab version of:

- “All links lead to Rome” (goal node),
- with a graph of states where

  - some moves are _necessary_,
  - many moves are _optional but redundant_,
  - and a huge number are _pure dead ends_.

What we care about for Sterling is not “can we solve Rush Hour?” (a BFS/solver already does that), but:

1. **Can path algebra + edge-relative decay discover _landmarks_?**
   (States or substructures that almost all successful solutions pass through.)

2. **Can it _sculpt_ the state-space so that “good routes” are reinforced and dead branches quietly fade?**
   (Exactly your Rome game intuition: “If I can just reach X, I know I can get to Rome in ≤ N steps.”)

3. **Can we compress the local mess into higher-level semantic nodes without losing navigability?**
   (“This whole sub-tree is: ‘free the red car’s front corridor’ as a conceptual operation.”)

A 5×5 Rush Hour instance is perfect because:

- The state space is big enough to show structure, but small enough that you can:

  - Enumerate or at least BFS it,
  - Visualize it like in your screenshots,
  - Run repeated episodes cheaply.

- The goal semantics are trivial and unambiguous (“red car at exit”), so we’re not fighting language fuzziness while we debug the memory dynamics.

---

## How to use the 5×5 graph to push Sterling forward

Here’s how I’d structure this as a progression of experiments that actually exercise your new `PathAlgebraEngine`, edge decay, and compression story.

### Phase 1 – Bake the Rush Hour domain into the KG

**Goal:** Treat Rush Hour states as first-class Sterling nodes with operators, not “just” a search demo.

**Concrete steps:**

1. **Define node + edge types for this domain**

   - Node type: `puzzle_state`

     - Metadata: board encoding, heuristic features (e.g., cars blocking red car, distance to exit).

   - Edge type: `legal_move`

     - Metadata: which car moved, direction, distance, cost (usually 1).

2. **Instantiate `NodeState` / `EdgeState` on top of this:**

   - You can reuse `NodeState`/`EdgeState` from `path_algebra.py` with:

     - `NodeRole.GLOBAL_ANCHOR` for:

       - “Goal reached” node,
       - Maybe a small set of canonical “milestone patterns”.

     - `NodeRole.SCRATCH` for ordinary puzzle states.

3. **Build a small but complete state graph for 5×5:**

   - Pick a starting puzzle that:

     - Isn’t trivial,
     - But has a unique or small set of shortest paths.

   - Run your existing BFS solver to:

     - Generate all states reachable from start (up to some depth cap),
     - Log (parent_state, child_state, move) tuples.

   - Load that into `FullKG` so that:

     - Each state is a node,
     - Each move is an edge with an `EdgeState` attached.

**Acceptance:** You can call a `kg.query_shortest_path(start, goal)` that returns the same optimal path as your BFS implementation, operating entirely via the KG abstraction.

---

### Phase 2 – Turn path algebra on and compare to plain BFS

**Goal:** Show that edge-relative weighting + SWM selection changes search behavior in the _right_ way.

You already have node salience and SWM building. Now:

1. **Run many solution episodes from the same start:**

   - Episode = “run a search from start to goal using a given search policy”.
   - For now, use:

     - **Baseline:** plain BFS or uniform-cost over legal moves.
     - **Sterling policy:** follow edges in order of descending `EdgeState.weight` from `PathAlgebraEngine`.

2. **On each step of a Sterling episode:**

   - Call `update_edges_after_step(u, v, config)` with your `EdgeDecayConfig`.
   - Record:

     - Which edges were taken,
     - Which competing edges decayed.

3. **Metrics to log per episode:**

   - **Search efficiency:**

     - `nodes_expanded` vs BFS baseline.

   - **Path convergence:**

     - How quickly do the top-weight edges settle into a consistent solution route?

   - **Branch pruning:**

     - Fraction of edges whose weight drifts below `prune_threshold`.
     - How many of those ever appear on optimal paths?

If this is working, you should see:

- The same optimal path emerges repeatedly.
- Edges along that path accumulate high weight.
- Edges leading into “weird cul-de-sacs” get suppressed.

That’s your “ideal path” learning without touching a transformer.

---

### Phase 3 – Landmark discovery and compression

This is where the Rush Hour domain really starts to look like your Rome game.

**Goal:** Automatically discover intermediate states that behave like semantic “landmarks” and compress them into Tier-1 summaries.

1. **Detect candidate landmarks:**

   - Over many solved episodes, compute for each node:

     - `solution_frequency`: fraction of successful episodes whose path contains the node.
     - `betweenness`: how often the node lies on a shortest path between start and goal.

   - Mark nodes above a threshold as `landmark` candidates.

2. **Create summary nodes:**

   - For each cluster of nearby landmarks (or repeated subsequences):

     - Use `compress_episode()`-style logic to:

       - Create an `EPISODIC_SUMMARY` node representing “red corridor cleared” or similar.
       - Rewire some paths to go via the summary node as a macro-step.

   Concretely, you might:

   - Identify a sequence of moves that consistently appears as a block across solutions,
   - Represent that as a single “macro edge” in the KG,
   - Tag the original fine-grained nodes as compressed under that summary.

3. **Rerun search using summaries:**

   - Modify your solver to allow:

     - Either micro-steps, or
     - A single summary edge that jumps across that subsequence.

   - Measure:

     - Reduced path length in terms of _macro_ moves,
     - Reduced nodes expanded,
     - Whether the solver still finds optimal micro-paths when needed.

This tests exactly your intuition:

> “We can wrap a lot of local spinning into a landmark and use it as a re-entry point / anchor later.”

No transformer. Just graph structure and path algebra.

---

### Phase 4 – Edge decay and topic-hopping inside the puzzle domain

You can even get a miniature version of “multiple tasks with shared landmarks” _within_ Rush Hour:

- Treat different **starting boards** as different “topics”.
- Many of them may share sub-problems:

  - “Free a vertical car blocking the exit row”
  - “Clear a 2×1 block from the corridor”

**Experiment:**

1. Generate 5–10 distinct 5×5 puzzles that:

   - Share the same goal cell,
   - But differ in arrangement.

2. Solve each puzzle multiple times with Sterling + path algebra, in a _sequence_ of episodes:

   - A1, A2, A3, B1, B2, C1, A4, …

3. Apply your edge-relative decay config:

   - Edges that are helpful for multiple puzzles should retain weight.
   - Very puzzle-specific detours should decay when they stop appearing.

4. Measure:

   - **Cross-task landmarks:**

     - Which edges/nodes become high-weight across _multiple_ puzzles?

   - **Retooling speed:**

     - When you switch from puzzle A to puzzle B, how many episodes before Sterling’s top-weighted edges are dominated by B-relevant moves rather than stale A-specific ones?

This is a contained way to test “topic hopping” before a global language KG.

---

## How this feeds back into Sterling’s bigger picture

Once you can show, in this fully explicit state-space, that:

- Edge-relative decay + path algebra

  - Finds stable, efficient paths,
  - Discovers and reinforces landmarks,
  - Compresses local detail into reusable summaries,
  - Retools quickly when tasks change,

you’ve effectively proven the _control loop_ you want Sterling to use over a linguistic KG:

- Replace “puzzle state” with “UtteranceState / knowledge-state”.
- Replace “move car X” with “apply operator M_OP/C_OP/S_OP” or “invoke tool T”.
- Replace “goal cell” with “task success condition (proof found / spec written / bug minimized).”

Then the unified encoder and latent compression are just the _bridge_ between messy language and the clean graph semantics you already know how to reason over.

---

## Short, prioritized next steps

If you want a concrete checklist to act on next:

1. **Wire Rush Hour into `FullKG` + `PathAlgebraEngine`.**

   - State nodes, move edges, node/edge state objects, shortest-path check.

2. **Implement Sterling-guided search vs BFS baseline** over a single 5×5 puzzle.

   - Log `nodes_expanded`, path consistency, edge weights across episodes.

3. **Add landmark detection + summary nodes;** re-run search with macro edges.

   - Confirm navigability is preserved and search becomes cheaper.

4. **Introduce multiple puzzles and a topic-hopping schedule** to exercise decay.

   - Track which edges survive as cross-puzzle landmarks.

Once you’ve got that, you’ll have a very tangible demonstration that you’re _not_ just bolting more stuff onto a transformer, but building a separate “semantic navigation engine” that transformers can plug into as a surface-form module later.
