# Glossary of Sterling System Concepts

## Canonical Definitions

Sterling maintains single-source canonical definitions in `docs/canonical/` to prevent documentation drift. These are the authoritative sources for key concepts:

| Canonical File | What It Defines | Link |
|----------------|-----------------|------|
| `north_star.md` | Sterling's operational definition as a path-finding system | [View](canonical/north_star.md) |
| `core_constraints_v1.md` | The 11 architectural invariants (INV-CORE-01 through INV-CORE-11) | [View](canonical/core_constraints_v1.md) |
| `sterling_architecture_layers.md` | Four-layer architecture: Reasoning, Memory, Carrier, Realization | [View](reference/canonical/sterling_architecture_layers.md) |
| `light_vs_full.md` | **Superseded.** See [sterling_architecture_layers.md](../reference/v1/canonical/sterling_architecture_layers.md) for the four-layer replacement. | (quarantined) |
| `code32_bytestate.md` | Code32 identity atom, ByteStateV1 packed state, ByteTraceV1 evidence format | [View](reference/canonical/code32_bytestate.md) |
| `bytestate_compilation_boundary.md` | How domain payloads compile into ByteState; epoch transitions; dynamic domains handshake | [View](reference/canonical/bytestate_compilation_boundary.md) |
| `neural_usage_contract.md` | Rules for when neural components are permitted | [View](canonical/neural_usage_contract.md) |
| `evaluation_gates_v1.md` | Research targets that must be demonstrated (EVAL-01/02/03) | [View](canonical/evaluation_gates_v1.md) |

**Important**: When these definitions appear in other documents, they are included via canonical markers and verified by CI for byte-for-byte equality with the source files.

## Core Constraints (Summary)

<!-- BEGIN CANONICAL: core_constraints_v1 -->
Sterling enforces 11 architectural invariants that block merge/deploy if violated:

| ID | Constraint | Description |
|----|------------|-------------|
| INV-CORE-01 | No Free-Form CoT | No generative LLM chain-of-thought in the decision loop |
| INV-CORE-02 | Explicit State | All task state in UtteranceState + KG, not transformer KV cache |
| INV-CORE-03 | Structural Memory | Episode summaries + path algebra for long-horizon, not transcript prompts |
| INV-CORE-04 | No Phrase Routing | No phrase dictionary or regex-based routing; all routing via scored search |
| INV-CORE-05 | Computed Bridges | Cross-domain bridges computed at runtime, not static lookup tables |
| INV-CORE-06 | Contract Signatures | Landmark/operator signatures are typed contracts, not learned embeddings |
| INV-CORE-07 | Explicit Bridge Costs | Domain transitions carry explicit costs with hysteresis |
| INV-CORE-08 | No Hidden Routers | All routing decisions auditable via StateGraph; no secret bypasses |
| INV-CORE-09 | Oracle Separation | No future/oracle knowledge in inference inputs; only in training signals |
| INV-CORE-10 | Value Target Contract | Canonical value targets versioned and hash-verified |
| INV-CORE-11 | Sealed External Interface | External tools cannot mutate internal state except via governed operators with declared write-sets |

See [core_constraints_v1.md](canonical/core_constraints_v1.md) for full details.
<!-- END CANONICAL: core_constraints_v1 -->

## Evaluation Gates (NOT Invariants)

<!-- BEGIN CANONICAL: evaluation_gates_v1 -->
These are research targets that must be **demonstrated**, not architectural invariants that block experimentation. They are measured regularly and trigger investigation/roadmap correction if violated.

| ID | Gate | What It Measures | Failure Response |
|----|------|------------------|------------------|
| EVAL-01 | Compute Parity | Sterling matches/beats transformer baselines in compute per task | Investigation + optimization sprint |
| EVAL-02 | Long-Horizon State | All long-horizon state lives in KG + summaries, not prompts/transcripts | Architecture review |
| EVAL-03 | Reasoning Substrate | Sterling demonstrably replaces LLM reasoning, not wraps it | Roadmap correction |

**Why gates, not invariants**: Invariants (INV-CORE-xx) are hard constraints that block merge/deploy. Evaluation gates are measured regularly and trigger corrective action, but don't make early iteration impossible. You can't require "already won" as a precondition for experimentation.

See [evaluation_gates_v1.md](canonical/evaluation_gates_v1.md) for full details.
<!-- END CANONICAL: evaluation_gates_v1 -->

## Neural Usage Contract (Summary)

<!-- BEGIN CANONICAL: neural_usage_contract -->
**Core Principle: Neural is advisory; Symbolic is authoritative.**

Neural signals MAY be used for:
- IR intake (text -> initial state / claims / spans)
- Explanation generation (rendering auditable path into text)
- Compression/indexing (latent embeddings, encoder hidden states) - **non-authoritative**
- Value estimation (scoring branches) - **non-authoritative**
- Ranking/prioritizing already-legal symbolic moves (hybrid value function)

Neural signals may NOT:
- Create new operators
- Bypass operator preconditions
- Mutate KG / UtteranceState directly
- Introduce new facts
- "Explain away" failed symbolic checks
- Override symbolic logic (only advise on move ordering)

See [neural_usage_contract.md](canonical/neural_usage_contract.md) for full details.
<!-- END CANONICAL: neural_usage_contract -->

---

## State Representation and Dynamics

**StateGraph:** The core data structure that captures the search space of reasoning as a graph of state nodes connected by operator applications (edges). The StateGraph is **append-only and deterministic**, meaning once a state and transition are added, they are never altered or removed. This ensures **auditable, reproducible reasoning** – all transitions can be replayed and verified. In Sterling, every problem is solved by **graph search** over states: an input is converted to initial **UtteranceState** \+ **WorldState** (context) + a goal, then the engine performs a StateGraph search for a state that satisfies the goal, finally yielding an output. The StateGraph provides a complete trail of how the solution was reached, satisfying strict determinism and auditability invariants.

**UtteranceState vs. DiscourseState vs. DialogueState:** An **UtteranceState** represents the annotated state of a single user utterance or sentence. It contains layered linguistic information (syntax, semantics, etc.) for that one unit of text. Sterling enforces **canonical utterance granularity**: one UtteranceState per sentence or snippet. In multi-turn dialogues or document-length reasoning, multiple utterance states are combined into a **DiscourseState**, which is a higher-level state representing a whole conversation or text. The DiscourseState **wraps multiple UtteranceStates** along with discourse-level metadata, such as rhetorical relations between utterances (e.g. question-answer, elaboration) and coreference chains (tracking entities across utterances). This allows Sterling to maintain **coherence and context** over a dialogue: each utterance is a part of a larger discourse structure. Additionally, in dialogue tasks the system tracks a **DialogueState**, which can be seen as part of the WorldState for conversational context. The DialogueState includes dynamic dialogue-specific info like the current conversation **phase or intent satisfaction** status. Importantly, some operators (especially in dialogues) might not change the UtteranceState’s content but will update the DialogueState (for example, marking an intent as fulfilled). These changes are captured as **observations** rather than semantic deltas (see **ObservationIR** below). In summary, UtteranceState is per-sentence, DiscourseState handles multi-utterance linking (cohesion and context), and DialogueState refers to the environment’s conversational status (used for goals and tracking progress in a dialog).

**CommitIndex:** A monotonic counter that indexes each committed operator application in an episode’s log. Every time an operator is successfully applied (an **OperatorApplication** is emitted to the StateGraph), the engine increments the commit index. This provides each transition with a unique sequence number, enforcing an **“emission contract”**: no gaps or out-of-order events in the recorded sequence of operations. The commit index only advances when an operator is actually applied to produce a successor state. If an attempted operation is disallowed or fails a pre-check (e.g. violates a constraint), it will **not** increment the commit index, preventing “holes” in the sequence. This invariant, introduced in the v1/v2 core integration, ensures consistent episode logs and makes it easy to reconstruct the exact sequence of reasoning steps. The commit index is crucial for auditability and aligning evidence with each step of reasoning.

**Episode Persistence:** Sterling logs each reasoning episode (from initial state to goal or failure) in a persistent, verifiable format. All intermediate states, applied operators, and evidence are recorded so that the episode can be replayed or inspected after the fact. This **“closure-ready” persistence** means that nothing is lost – the entire search and outcome can be serialized into an episode log artifact. Such episode logs feed into the induction pipeline and certification process. Importantly, the logs are structured so that they can be used to **close the loop**: induction processes or external auditors can take the log and verify outcomes or derive learning from it. In Sterling’s design, **every piece of evidence from search is persistable**, enabling later stages (like operator induction or proof bundling) to use the exact data. Episode persistence also supports offline analysis and debugging, and it allows the system to **accumulate experience** over time. In Sterling v2, persistent episodes (especially failure episodes or near-misses) serve as training data for the inductive reasoner.

## Operators and Actions

**Operator Calculus (S/M/P/K/C Operators):** Sterling defines a formal **operator calculus** — a taxonomy of operators divided into five categories: **Seek, Memorize, Perceive, Knowledge,** and **Control** (Sterling Native canonical names; v1 documentation used "Structural, Meaning, Pragmatic, Knowledge, Control" — see ADR 0004). Each operator is a transformation on state that falls into one of these intents:

- **Seek (S):** explore or navigate state space (expansion, neighbor enumeration, probing candidates).
- **Memorize (M):** commit, consolidate, summarize, or otherwise update durable state (landmarking, retention events).
- **Perceive (P):** interpret observations, update beliefs, or incorporate new evidence.
- **Knowledge (K):** query or extend world knowledge within the governed substrate.
- **Control (C):** manage search flow (budgeting, selection control, stopping, orchestration operators).

Crucially, **each operator has a declared signature with preconditions and effects** (a contract). Sterling enforces these contracts at runtime — an operator can only apply if its preconditions are satisfied, and it may only modify state within its declared write-set. This yields **auditable, deterministic state transitions**: every state change is attributable to an operator application that satisfied a contract. The operator calculus ensures **composability** (operators can chain in reasoning) and prevents side effects. All S/M/P/K/C operators are first-class in the system — they are registered in an **OperatorRegistry** with their metadata, making their definitions queryable and their use trackable.

**Operator vs. Operator Application:** An **Operator** in Sterling is an abstract, reusable definition of an action — it includes the operator’s category (S/M/P/K/C), its precondition logic, and its effect function (how it transforms a state). Think of it as a **rule or operation schema**. An **Operator Application**, by contrast, is a specific event when an operator is applied to a particular state, producing one or more successor states. For example, if “ExpandNeighbors” is an operator (Seek category), then applying it at step 5 on state `X` is an operator application. Sterling treats operator applications as distinct entities in the trace: each application gets an entry with details like which operator was used, input state, resulting state, and the commit index of that step. This distinction is important for logging and learning — multiple applications of the **same** operator (on different states or at different times) will share the operator definition but are separate events in the episode log. Operator definitions are **certified and stored** in the registry, whereas operator applications are transient events that occur during reasoning. The system’s governance rules often apply at the moment of operator application — for example, fences (allowed operator lists) or gates will decide **at application time** whether a given operator is permitted in the current context. In summary, an **Operator** is like a blueprint (the _what_ and _when it’s allowed_), while an **Operator Application** is a single execution of that blueprint (the _when/where it was used_ in a reasoning trace).

**Certified Operator:** A **certified operator** is a new operator that has been **learned and verified** through Sterling’s induction and certification pipeline before being promoted to the production operator set. Induced operators (hypotheses the system generates to handle new problems) aren’t used in normal reasoning until they go through a rigorous certification process. Certification involves proving that the new operator provides a beneficial capability **without introducing regressions**. The certification process produces an **artifact (certificate)** which includes evidence of the operator’s performance, validity of its preconditions/effects, and verification that it adheres to system invariants. Only with this certificate can the operator be **promoted** (added to the official operator registry for general use). Certified operators are trustworthy extensions of Sterling’s capabilities — they come with proof bundles and decision records that can be independently verified. At runtime, a certified operator is treated like any other operator, but its origin is documented (it was induced from data and then formally admitted). Promotion to the production operator set is **certificate-driven and gate-verified** — meaning an explicit approval (the certificate) is needed, preventing arbitrary learning from corrupting the system. Essentially, _certified operators_ represent **new skills the system has learned** in a controlled, auditable way.

## Learning and Induction

**Dual-Stream Evidence:** Sterling’s induction subsystem relies on a **dual-stream evidence substrate** to learn new operators and improve its reasoning. The two streams are: **(A) Semantic Deltas** and **(B) World Observations**. Stream A captures _what changed in the semantic content_ of the utterance when an operator was applied, while Stream B captures _what happened in the world or dialogue_ as a result of the state transition. By combining these two, Sterling can learn from both internal linguistic changes and external outcomes. The design acknowledges that **no single type of evidence is sufficient**. For instance, a dialogue operator might produce no semantic change (no new information in the utterance state) but still achieve something meaningful, like completing a dialog phase – this would register as an observation in Stream B. Conversely, some state changes have no immediate external signal but are meaningful internally. **Dual-stream evidence** ensures the induction process sees the full picture: _what was the system’s internal action and did it matter to the goals?_ Stream A (deltas) and Stream B (observations) together link **“what changed”** with **“why it mattered”**, which guides learning. This substrate feeds into the creation and evaluation of hypotheses. It ties each operator application to both a semantic diff (via **SemanticDeltaIR**) and a success signal or event (via **ObservationIR**), allowing Sterling to reason about causality: _did a certain change help achieve a goal?_ In practice, dual-stream evidence is logged per step and later aggregated to propose new operators that could replicate successful patterns.

**SemanticDeltaIR:** The **Semantic Delta IR (Intermediate Representation)** is a structured representation of the **change in the UtteranceState** caused by an operator. Whenever an operator is applied, Sterling computes a “delta” – essentially a diff or edit – describing how the utterance’s semantic content was transformed. This could be, for example, a new predicate added, a semantic role filled, or some part of the meaning updated. The SemanticDeltaIR is the artifact encoding that change in a machine-readable form. It serves as **evidence** for learning: if a particular semantic delta frequently correlates with achieving a goal, the induction engine may generalize that into a new operator hypothesis. Notably, the system has a governance option `require_semantic_delta` (see that term) to enforce that meaningful semantic deltas must be present for each operator application – ensuring we don’t accept successor states where no semantic content change was captured. The SemanticDeltaIR stream (also called Stream A) is half of the dual evidence backbone. It focuses purely on the _utterance-level transformation_. For domains where actions always change the utterance state, this delta alone might be enough. But in domains like dialogue (where utterance text might remain the same but context changes), a semantic delta could be empty, which is why the ObservationIR is needed in parallel. In summary, **SemanticDeltaIR** provides a formal, diff-like record of “what changed in the sentence’s meaning” at each step, feeding the induction algorithms that search for patterns of effective changes.

**ObservationIR:** The **Observation IR** is the representation of **external or high-level observations from a state transition**, typically changes in the WorldState or discourse that are not reflected directly in the utterance’s semantic content. This is Stream B of the dual evidence. Examples of ObservationIR include events like “an intent was fulfilled”, “moved to next dialogue phase”, “goal condition met”, or any domain-specific world feedback that occurred after an operator was applied. In dialogues, for instance, operators often produce **world observations** such as a flag that the user’s question was answered or that a constraint in the environment was satisfied. These observations are critical for learning because they connect actions to **outcomes**. The ObservationIR encodes things like state transitions in the environment, reward signals, or success/failure indicators in a structured way. Together with SemanticDeltaIR, it lets the system correlate _internal changes_ with _external results_. If an operator causes no semantic delta but triggers a positive observation (say, completing a task), that evidence is captured solely by the ObservationIR. The induction engine uses ObservationIR to identify which aspects of state transitions lead to satisfying the goal. Sterling’s architecture mandates that if a domain can have state changes without altering utterance content, it **must emit an ObservationIR** to document that. This way, nothing that matters for learning is invisible. Overall, **ObservationIR** is the mechanism for recording **“what happened in the world/dialogue because of this step”**, and it is as important as semantic deltas for training the inductive reasoner.

**Hypothesis Lifecycle:** In Sterling v2 (the inductive reasoner), a **hypothesis** is a proposed new operator or rule that might explain how to reach goals more effectively. Hypotheses are treated as **first-class graph objects with their own provenance and lifecycle**. The lifecycle of a hypothesis typically goes through stages: **generation** (from analyzing evidence in episodes, the system posits a candidate operator that could generalize the successful pattern), **evaluation** (the hypothesis is tested across multiple scenarios or examples to see if it holds true beyond the original case), **invariance checking** (verifying the hypothesis is consistently effective and respects domain invariants), **optimization** (maybe refining it for simplicity or stronger preconditions), and finally **certification or rejection**. During this lifecycle, Sterling attaches provenance data to the hypothesis – which episodes or outcomes inspired it, what evidence supports it – enabling traceability. A hypothesis might start as an “operator sketch” (an approximate idea of what an operator does, perhaps learned from data). It then must survive strict **generalization gates**: Sterling requires **invariance checks** to pass (meaning the hypothesis works on multiple varied instances, not just one specific case). It also uses **MDL/parsimony** (see below) to favor simpler hypotheses. If a hypothesis passes all tests (no contradictions, broad enough applicability, minimal complexity) it can be turned into a **certified operator** through the formal certification pipeline. If it fails (e.g. a counterexample is found, or it’s too complex relative to its benefit), it’s discarded or sent back for revision. The key point is Sterling doesn’t instantly trust any learned rule – hypotheses live in a controlled environment where they are proven or disproven. They have a clear **lifecycle from birth to either promotion or elimination**, with safeguards (like invariants and MDL) at each step. By the time a hypothesis becomes a certified operator, it has an audit trail of why it was created, how it was validated, and evidence of its utility.

**Invariants (Induction Invariants):** In the context of learning, **invariance checks** refer to the requirement that a candidate induced operator captures a genuine generalization, not an accidental quirk of one example. Sterling enforces that any hypothesis must satisfy **cross-example invariants** – essentially, it should work consistently across different instances of a problem class. For example, if the hypothesis is a rule to solve a puzzle, it should solve all versions of that puzzle type, not just the one seen. Invariance checks are treated as **hard gates on generalization**. Practically, the system might generate multiple scenarios or use held-out variations to test the hypothesis. Only if the success holds invariant under those tests does the hypothesis progress. Some invariants are logical (the operator should not violate known logical laws or domain constraints), and some are empirical (it should succeed on a diverse set of cases). Additionally, Sterling maintains system-wide invariants (like determinism, no hidden state) which any new operator must also respect. In the induction pipeline, invariants prevent **spurious hypotheses** – e.g. ones that overfit to noise or a single trick. There is also mention of **identity invariants** in governance (like ID-0: no derived identifiers in hashes), but in learning context, invariants mainly mean _generalization conditions_. In short, **invariants** here are the non-negotiable conditions a hypothesis must meet to be considered valid broadly, ensuring new operators reflect true regularities of the task and not coincidental patterns.

**MDL / Prior Parsimony:** Sterling employs a **Minimum Description Length (MDL)** principle as a prior to favor simpler explanations (operators) over complex ones when learning. MDL is a formalization of Occam’s Razor – among hypotheses that explain the data, prefer the one with the shortest description (or lowest complexity). In Sterling’s induction module, MDL acts as a **selection criterion and tie-breaker**: if multiple candidate operators pass the invariance tests, the one that provides the best trade-off of coverage vs complexity (i.e., more parsimonious) is chosen. For example, an operator that achieves the goal with fewer preconditions or simpler logic might be favored over an overly specific operator, even if both succeed on the examples. This is implemented as part of the **operator selection rules** in K6 proofs, where they pre-register a deterministic selection rule that uses MDL cost, success rate, and other factors. By enforcing a parsimony prior, Sterling avoids bloating the system with highly specialized rules that don’t generalize. The **prior** in this context means the system is biased to assume simpler hypotheses are more likely to be true, unless evidence strongly suggests a more complex one. MDL also contributes to efficiency: learned operators that are simpler often require less computation to match/apply. In summary, **MDL/prior parsimony** ensures the inductive learner doesn’t just find any explanation for new data, but finds the **most elegant** one that fits, thereby improving generalization and maintaining a compact reasoning system.

## Certification and Benchmarking (K6 Framework)

**K6 Benchmark Suite:** **K6** is the codename for Sterling’s **certification-oriented benchmarking framework** that validates the end-to-end induction process. It is designed as a rigorous test harness to prove that a newly induced operator actually delivers a **capability improvement under controlled conditions**. The K6 framework runs a special scenario in three configurations – often called **Config A, B, and C** – to isolate the effect of a new operator. The objective (sometimes called the K6 proof) is: _demonstrate that promoting at least one certified induced operator yields measurable performance gains on a scenario that current operators cannot solve, with no regressions, and produce a replayable proof artifact_. In practice, a **K6 scenario** is a challenge problem set up for induction. The framework first does a **Scenario Gate** (pre-check) to ensure the scenario is truly unsolvable by the existing system (so any success later can be attributed to the new operator). Then it executes three runs: **A (Baseline)**, **B (Shadow)**, and **C (Promotion)**.

* _Config A_ is the baseline run using only the current production operators. It establishes a benchmark (typically expecting failure or very low success on the new scenario).
* _Config B_ is the shadow run: the candidate operator is loaded in a **shadow mode** where it can influence reasoning heuristics or suggestions but is **not allowed to actually apply as a state transition**. This tests that the mere presence of the new operator (for e.g. scoring or hints) doesn’t inadvertently solve the problem – i.e., ensures any improvement is due to actually using it, not just leaking information. The K6 framework enforces that outcome B should not significantly exceed A (if B performed much better, that indicates a “leak” where the shadow operator is accidentally being used to solve the task). This is called the **Shadow Non-Causality Gate**: B’s metrics must approximately equal A’s.
* _Config C_ is the promotion run: the new operator is fully **promoted** (i.e., allowed to be applied in search) for this run, in addition to all existing operators. This run should show a **measurable improvement** (e.g., higher success rate or better metric) over A, demonstrating the new operator’s value. It must also show no regressions on any aspect (if there are multiple metrics, C shouldn’t be worse than A on secondary metrics, for example).

All three runs are executed under identical conditions apart from the operator’s availability. K6 includes strict controls: each run is in a **fresh engine instance with identical seeds and configs** (only difference is the operator and its mode). This isolation ensures scientific rigor – results are directly comparable without cross-contamination. The output of a K6 proof is a **Proof Bundle** (see below) that contains all data to verify the claim. In essence, **K6** is the mechanism by which Sterling “proves” a learned operator is truly an improvement. It’s not just a unit test; it’s an end-to-end demonstration including governance checks (like identity invariants, see **Governance Mechanisms**). The name “K6” references an internal milestone (the sixth Key outcome in a series, presumably) focusing on induction and certification. It’s accompanied by a set of K6-_invariants_ and rules (K6-G1, K6-E1, etc.) that codify its requirements. By running K6, developers and stakeholders gain high confidence that any new operator promoted by induction is beneficial, reproducible, and safe.

**Config A / B / C Runs:** These refer to the three specific run modes within the K6 benchmarking protocol:

* **Config A (Baseline):** Run the scenario with the current **production operator set only**, and no influence from any new (induced) operators. This establishes the **baseline performance**. For a valid induction scenario, Config A should essentially fail (e.g., success rate \~0%). If A can already solve the scenario above a tiny threshold (ε), then the scenario does not require a new operator and is disqualified.
* **Config B (Shadow):** Run the scenario with the new operator loaded in _shadow mode_. In shadow mode, the operator can be used for analysis or scoring but **cannot actually be applied to generate new states**. This is enforced by the engine via an **apply fence** that excludes the shadow operator’s ID from allowed applications. The purpose is to check that just having the operator’s logic around doesn’t accidentally solve the problem (for example, by altering heuristic scores or providing hints). Ideally, Config B’s outcome should mirror A’s (no significant improvement), confirming the new operator itself (when not applied) isn’t leaking a solution.
* **Config C (Promotion):** Run the scenario with the new operator **fully enabled (promoted)** as if it were part of the production set, but only for this run. In C, the operator can be applied to reach new states. This should show a **positive result**: if the operator is truly useful, Config C will solve or perform better on the scenario where A and B failed. The difference between C and A (with B as a control) isolates the operator’s causal impact. Config C is carefully managed: the new operator is loaded into an **overlay registry** (temporary promotion) so that it doesn’t persist beyond this run.

Using A/B/C together provides a mini-experiment: A vs C shows the benefit, A vs B ensures no hidden confounders, and B vs C confirms the operator’s direct effect. All runs use the same initial conditions and random seeds (Sterling ensures bit-for-bit reproducibility with seeded determinism). The K6 suite often repeats A/B/C over multiple random seeds to get statistically sound results (e.g., N≥5 runs each). Only if **C consistently outperforms A, while B \~ A**, can we declare the operator effective and isolate its contribution.

**Scenario Gate:** A **Scenario Gate** is a precondition check on any scenario to be used for induction proofs like K6\. It ensures the scenario is legitimately one that requires a new solution. Concretely, the scenario gate involves running **Config A** (baseline with existing operators) multiple times to measure the success rate. If the baseline has more than a trivial success rate (even a few percent), the scenario is considered **invalid for induction** – because it’s already solvable by what we have. Formally, Sterling defines a threshold ε (like 0% or 5%) and runs A over N different seeds; if `success_rate_A > ε`, the **gate fails**. The rationale is to avoid “inducing” an operator for a problem that didn’t need induction. Scenario Gate is essentially a **stop-the-line check**: it _blocks_ the entire induction proof attempt if the scenario doesn’t meet the unsolvability criterion. This saves time and prevents false claims. Only after a scenario passes the gate (A fails consistently) will the system proceed to run A/B/C with a candidate operator. In practice, scenario authors are expected to design scenarios that deliberately exploit a gap in the current operator set. The gate is an automated verification of that assumption. It adds integrity to the K6 process by guaranteeing the “improvement” isn’t just solving something that was already occasionally solved by luck or existing knowledge. So, **Scenario Gate** \= “Baseline must demonstrably fail this scenario, otherwise no induction needed”.

**Fence (Apply Fence) Enforcement:** A **fence** in Sterling is a mechanism to strictly control which operators can be applied during a run. The **ApplyFence** is essentially an allowlist (or a set of allowed operator IDs) that the engine checks **before executing any operator**. Under the K6 framework and general governance, fence enforcement ensures that in certain configs or scenarios, forbidden operators are not used. For example, in Config B (shadow mode), a fence is used to exclude the shadow operators from actual application. The fence can run in different modes: **STRICT, WARN, or OFF**. In STRICT mode, if an operator not on the allowlist is about to be applied, the engine will **block and fail** (fail-closed). In WARN mode, it would allow it but log a violation warning. OFF would disable the check (not typically used in formal proofs). The fence is derived from a set of operator IDs (often coming from an artifact, see **AllowedApplySetV1**). During a fenced run, the search’s **primary choke point** is wrapped with a fence check – before any state transition, the system verifies the operator’s ID is permitted. Defense-in-depth: even lower-level calls like the OperatorRegistry’s `apply()` double-check the fence in case an operator is invoked outside the normal path. This thorough enforcement prevents any “sneaking through” of disallowed actions. In K6, fence enforcement is crucial for maintaining experimental control (especially K6-E3 invariants about no unintended use of shadow ops). More broadly, fences can be employed for governance – e.g., running the engine with a fence that only allows _certified_ operators, or imposing a policy mode where some experimental ops are barred. The result of a fenced run is often encapsulated in a **FenceResult** which records if any illegal operator was attempted, among other metrics. In summary, **fence enforcement** is Sterling’s way to impose a **hard governance boundary** on operator usage, ensuring that in sensitive runs, only the intended operators can influence the outcome. This guarantee is fundamental to certification experiments and secure deployments.

**FenceRunConfig:** A configuration object defining the parameters and policies for a fence-governed benchmark run. It encapsulates how the engine should behave under fenced conditions. The `FenceRunConfig` (also referred to as `FenceConfigV1` in newer docs) typically includes fields like the run mode (A, B, or C), random seed, whether certain constraints are enabled, and what enforcement mode to use. For example, a FenceRunConfig might specify `mode="C"`, `seed=42`, and `enforcement_mode="STRICT"`. It also holds references to artifacts like the allowed operator set or a policy snapshot (for ensuring identical conditions across runs). In code, this is a dataclass with fields such as `run_config_id`, `enable_constraints` (to enforce domain-specific invariants during the run), a guidance regime (which hints how the heuristic should behave, e.g. whether to use neural guidance or not), and the enforcement level for the apply fence. The purpose is to have a **stable, reproducible config** that can be hashed or recorded, so that the run can be replayed exactly as it was. In K6 proofs, each of A, B, C is executed with a corresponding FenceRunConfig (or FenceConfig) that differs mainly in the mode and which operator set is loaded. The config is often included in the Proof Bundle to document how the run was set up. In summary, **FenceRunConfig** is the formal specification of a fenced benchmark execution – controlling operator allowlists, constraint toggles, and ensuring consistency across repeated runs.

**AllowedApplySetV1:** This is a data artifact that enumerates the exact set of operator IDs permitted to be applied in a fenced run. Essentially, it’s the saved allowlist (and its hash) used by the ApplyFence. When Sterling runs a fenced scenario (like in K6), it generates an AllowedApplySetV1 which contains a sorted list of operator identifiers that were legal in that run. By storing this as an artifact, the system can later verify that the run did not stray – the trace of applied operators can be checked against the allowed set. It also aids determinism: by reusing the same AllowedApplySet, one ensures the fence is exactly the same on re-run. The content typically includes both the production operator IDs and any inducted operator ID (for config C) minus any forbidden ones (like shadow ops in config B). The artifact’s description in the code is: “stores the exact allowlist configuration used during application to enable deterministic replay and verify hash integrity.”. Hash integrity is important; the AllowedApplySet’s hash might be part of the run manifest to ensure no tampering (if the allowed set changed, the hash would differ, invalidating the proof). In simpler terms, **AllowedApplySetV1** is a record of “which operators did we permit?” in a given test, used for both controlling the run and later validation. It’s part of the **fence evidence** saved in the Proof Bundle, demonstrating that (for example) in Config B none of the shadow operators were allowed or used. This contributes to the independently verifiable nature of Sterling’s certification pipeline.

**Proof Bundle:** In the context of induction and certification, a **Proof Bundle** is the collection of all artifacts, data, and results from a K6 proof run, packaged together for external verification. When Sterling completes a K6 scenario (A/B/C runs), it produces this bundle to serve as evidence for the success and safety of the induced operator. A typical proof bundle includes:

* The **scenario specification** (task definition, inputs, expected outputs, and any train/held-out split used).
* The **run manifests** or config files for each run (documenting seeds, config hashes, policy versions, etc., ensuring reproducibility).
* The **AllowedApplySet artifacts** for each run (see above) and any **RunManifest** or config hash to prove each run’s conditions.
* The **certificates** themselves (digital or data artifacts certifying the new operator and the decision to promote it). This might include the operator’s definition and test results, signed or hashed to prevent alteration.
* The **A/B/C results artifacts**: e.g., traces or summaries of each run’s outcome, performance metrics, maybe logs of state transitions, success/failure counts.
* Possibly a **Proof Report** – a human-readable markdown or PDF summarizing the causal claim (that the new operator caused improvement), including tables of metrics and any invariant checks performed.

The idea is that someone else (or an automated auditor) could take the proof bundle and **independently replay** the runs or at least verify the hashes and results to confirm everything. The proof bundle makes the induction outcome **replayable and transparent**. For example, it should be possible to re-run the engine with the given manifest and allowed set and see the same outcomes. It also serves as documentation for future reference (like which operators were inducted and why). In short, the **Certified Proof Bundle** is the deliverable of the K6 process: a certified, self-contained evidence package that a new operator is safe to promote and indeed brings a benefit.

**Ephemeral Promotion (Overlay Registry):** In Sterling’s certification runs, **operator promotion** must be done in a controlled, non-persistent way. **Ephemeral promotion** means the new operator is only added to the engine’s operator set **temporarily for the scope of one run**, rather than permanently. The mechanism for this is an **Overlay Registry**, which sits on top of the base operator registry. In Config C, for example, when we want to test a certified operator, the system creates a fresh registry instance that starts as a copy of the production registry and then **overlays** the new operator into it. Any operator in the overlay is available for that run, but after the run the overlay is discarded. This approach ensures two things: (1) the new operator doesn’t accidentally leak into other runs like A or B (each run has its own isolated registry), and (2) even after Config C, the global production registry remains unchanged unless we explicitly decide to promote the operator after successful proof. The overlay registry forwards most lookups and calls to the base registry, except when an operator ID is in the overlay (then it uses the overlay’s definition). It also can list all operators (base + overlay) for logging and fence setup. Sterling explicitly flags that **persistent promotion is a “loaded footgun” to avoid** – under no circumstance should Config C’s promotion persist beyond the run. By using ephemeral promotion, K6 maintains experiment purity and avoids contaminating the system state. In summary, **operator promotion constraints** in K6 mean: promote the induced operator in a _sandboxed_ way for evaluation, never directly into the long-term system during testing. Only after the proof passes would one merge it into production, and even then via the certificate process. This concept keeps induction trials safe and reproducible.

## Governance and Integration

**Governance Mechanisms:** Sterling has a strong **governance layer** designed to enforce rules and invariants that preserve system integrity, especially as learning and induction come into play. Key governance mechanisms include **identity invariants, certified artifact checks, and fail-safe defaults**. For identity, Sterling follows the **ID-0 rule**: no derived identifiers (like operator IDs or state IDs) are allowed to incorporate unpredictable data (like memory addresses or timestamps) – they must be stable hashes of content. This ensures that all artifacts (episodes, operators, certificates) can be hashed and verified independently across runs. Another governance aspect is the **event certification**: promotion events and proof runs themselves are signed off by decision records, meaning there’s an auditable record anytime something enters production. Perhaps the most important governance mechanism is the concept of **sealed operator application** using contexts and gates. In the latest design, an immutable **GovernanceContext** is created for each session, embedding the rules under which operators will execute. This context carries things like the allowed operator universe (via fences), the precondition resolution policy (strict vs backward-compatible), and evaluation mode (STRICT or WARN). In effect, it _seals_ the reasoning loop so that if an operator violates a rule (say, tries to apply when it shouldn’t), the system will catch it immediately. **Fail-closed enforcement** is a guiding principle: by default, if something abnormal is detected, Sterling will stop or drop that operation rather than proceed in an undefined state. This applies to things like missing semantic deltas (see below) or evidence gaps (cheating detection will fail an execution if required evidence isn’t recorded). Additionally, governance covers **policy snapshotting** – before an induction run, the current policy (operator set, selection rules, etc.) is hashed and recorded so that any drift or difference is detectable. In summary, Sterling’s governance mechanisms are the collection of **checks and balances** that ensure:

* Identity and hash consistency (for reproducibility and security).
* Only authorized changes happen (promotion requires certificates; unapproved operators can’t run due to fences).
* All necessary data is captured (no silent failures).
* The system operates in a deterministic, auditable manner (no hidden randomness or state bleed-through).

This governance layer was significantly fortified in late 2025 with the induction integration, adding things like the K6 governance invariants (e.g., K6-G1 requiring pre-registered selection rules, no post-hoc cherry picking) and K7 which introduces an always-on apply fence and strict precondition resolution to eliminate any ambiguity at runtime. Collectively, these mechanisms guard Sterling from both inadvertent errors and deliberate misuse, ensuring the system “fails safe” and that any learning is rigorously vetted.

**`require_semantic_delta` (Enforcement Mode):** **require\_semantic\_delta** is a governance flag that can be enabled in Sterling’s execution context to enforce that every successful state transition produces a valid **SemanticDeltaIR**. When this flag is **True**, the engine performs a _preflight check_ after an operator is applied: it computes the semantic delta for the resulting state and verifies the delta is not empty or invalid. If the semantic delta computation **fails or results in no change**, the successor state is **discarded** (the operator application is treated as illegitimate). In effect, `require_semantic_delta=True` means the system will **fail closed on any operator that doesn’t yield a semantic change**. This mode was introduced in Phase 4.5 of development as part of tightening the induction data quality. The rationale is that for learning (and for transparency), we want to avoid transitions where nothing in the utterance state changed, as those can indicate a no-op or an operator whose effects weren’t properly captured. Especially in induction runs, having every step carry a meaningful semantic delta simplifies evidence collection (every OperatorApplication then has an associated delta artifact). It is recommended to turn this on for induction and evaluation scenarios, so that any operator that would create a “hole” in the evidence (no delta) is automatically filtered out as an invalid successor. There are rare cases where an operator intentionally should not change the utterance (for instance, a pure control operator that only affects the world state), but the design philosophy is that such cases should instead produce at least an ObservationIR and likely a trivial semantic marker, or be explicitly exempt. Optionally, if the flag is False, the engine would allow delta-less transitions but could label them as such (a not-implemented option was to mark them with a status instead of dropping). However, in strict governance mode, dropping them is preferred. Overall, **`require_semantic_delta`** is a switch that guarantees **no state transition is accepted without a corresponding semantic change record**, thereby maintaining the completeness of the dual-stream evidence and preventing hidden no-ops.

**Environment API (Stage O):** The **Queryable Environment API** (often referred to as Stage O in the roadmap) is a newly introduced interface that allows external agents or scenario harnesses to interact with Sterling’s internal state and artifacts in a controlled, high-level manner. Its goal is to provide **“stable, budgeted, cite-backed”** access to things like episode logs, state graphs, evidence records, hypotheses, and certificates. In simpler terms, instead of hooking directly into Sterling’s internal Python objects (which could change as the system evolves), an external component can use the Environment API to query, for example: “retrieve the last state where the user’s intent X was fulfilled” or “get me the evidence (deltas/observations) from episode #5” or “ask the environment if a given hypothesis has been certified.” Each query is handled through a defined protocol (possibly a DSL of query commands) and the results are returned in a standardized format. The **“cite-backed”** aspect means that when the environment returns data, it can include references or handles to the underlying artifacts, ensuring traceability (like references to which episode or state ID it came from). **Budgeted** means queries have cost limits – to prevent runaway requests or ensure performance, the API enforces limits (like time or resource budgets) on how much data can be pulled in one query. The Environment API was created to support integration with agent platforms and orchestration frameworks without exposing Sterling’s internals directly. For example, a dialogue agent could use the Environment API to fetch relevant context from past interactions (which are stored as episodes) or to submit a new query to be solved by Sterling’s reasoning engine. The roadmap explicitly notes that scenario harnesses should be built on this API, not by reaching into internal modules, to decouple them from changes in Sterling’s core. Additionally, every interaction via the API is an **auditable event** – meaning it gets logged, so you can later see what was asked and what was returned, preserving transparency. The Stage O API essentially acts as a **façade** or gateway, implementing a query language (`QuerySpec` DSL is hinted) that covers all needed operations to get info from Sterling’s world, without risking consistency or requiring the caller to know about dozens of classes. By January 2026, Stage O was marked **complete** with a unified `core/environment/api.py` and a battery of tests ensuring that slices of data returned are deterministic and correctly limited. In summary, the **Environment API** is Sterling’s answer to providing a safe, consistent way for external systems to **query its knowledge, states, and outcomes**, enabling integration (like plugging Sterling into a larger multi-agent system or a user-facing tool) while maintaining the integrity and encapsulation of the core reasoning engine.

## Sterling Full vs. Sterling Light *(superseded)*

> **This distinction is a v1 concept and does not carry forward to Sterling Native.** Sterling Native uses a single four-layer architecture (Carrier → State → Operator → Search) with governance modes (DEV / CERTIFIED) instead of variant splits. The v1 Full/Light taxonomy is archived at [`docs/reference/v1/canonical/light_vs_full.md`](../reference/v1/canonical/light_vs_full.md). See also [`docs/architecture/clean_sheet_architecture.md`](../architecture/clean_sheet_architecture.md) and [ADR 0001](../adr/0001-compilation-boundary.md) for the current architecture.
