id: TAPE-001
type: feature
title: >-
  Tape Writer Hot-Path Optimization: eliminate avoidable work during search
  recording
status: complete
risk_tier: 2
mode: development
created_at: '2026-02-27T03:03:20.350Z'
updated_at: '2026-02-27T03:03:20.351Z'
blast_radius:
  modules:
    - search/tape
    - search/search
    - benchmarks
  data_migration: false
operational_rollback_slo: 5m
scope:
  in:
    - search/src/tape.rs
    - search/src/tape_writer.rs
    - search/src/search.rs
    - harness/src/runner.rs  # lint allow on ScorerInputV1 (ContentHash size consequence)
    - benchmarks/
    - .caws/specs/TAPE-001.yaml
  out:
    - kernel/
    - search/src/node.rs
    - search/src/frontier.rs
    - search/src/graph.rs
    - search/src/contract.rs
    - harness/src/bundle.rs
    - harness/src/bundle_dir.rs
    - tests/lock/
    - docs/
    - ml/
scope_overlaps:
  - spec: SPINE-001
    files:
      - harness/src/runner.rs  # SPINE-001 includes harness/; TAPE-001 only added lint allow
    reason: >-
      TAPE-001 added #[allow(clippy::large_enum_variant)] on ScorerInputV1 in
      e8b1844. This remains necessary after the ContentHash cache revert because
      clippy still flags the Table variant size. The change is a single-line lint
      annotation that does not alter SPINE-001 harness semantics.
threats:
  - "Tape-only invariants gating the non-tape search path (coupling footgun)"
  - "Pre-sizing buffer from untrusted policy values causing OOM"
  - "Struct bloat in ubiquitous types (ContentHash) causing cache pressure regression — measured and reverted"
invariants:
  - "INV-TO-01: Tape recording is optional; search without tape must not fail due to tape-only invariants"
  - "INV-TO-03: TapeWriteError variants are tape-local; no kernel error types exposed in search public API"
  - "INV-TO-04: Scratch buffer provides transactional atomicity: if body construction fails, buf is untouched"
  - "INV-TO-05: Pre-allocation is clamped to 256 MB regardless of policy input"
acceptance:
  - id: TO1-NO-TAPE-NO-FAIL
    given: "search() called without tape (tape_sink is None)"
    when: "search completes"
    then: "no tape-related errors; search_fn_total within ±3% of pre-TAPE-001 baseline"
  - id: TO1-SCRATCH-ATOMICITY
    given: "TapeWriter with scratch buffer"
    when: "body construction fails (e.g., OpArgsTooLong)"
    then: "self.buf is unchanged from before the on_* call"
  - id: TO1-ERROR-LAYERING
    given: "TapeWriteError enum"
    when: "inspected for kernel type dependencies"
    then: "no kernel error types; content_hash_to_raw maps to tape-local InvalidHexDigest/UnsupportedHashAlgorithm"
  - id: TO1-PREALLOC-CAP
    given: "policy with max_expansions=u64::MAX"
    when: "estimate_tape_capacity() is called"
    then: "returns fallback (4096), not OOM"
  - id: TO1-SHORT-DIGEST-REJECTION
    given: "a ContentHash with sha256 algorithm but short hex digest"
    when: "content_hash_to_raw() is called"
    then: "returns TapeWriteError::InvalidHexDigest"
non_functional:
  performance: "No regression on search_fn_total; tape overhead reduced ~5% via scratch buffer + prealloc"
  security: "Pre-allocation clamped to 256 MB; no untrusted-size allocations"
  accessibility: "N/A (internal optimization, no UI)"
contracts:
  - type: project_setup
    path: .caws/specs/TAPE-001.yaml
    description: "Internal perf optimization; no external API contracts. Invariants defined in spec."
reverted:
  - change: "ContentHash raw_sha256 cache (Option<[u8;32]>)"
    reason: >-
      Struct bloat (~40→72 bytes) caused 18-23% regression in search_fn_total at
      scale_1000 via cache pressure on SearchNodeV1/CandidateActionV1. Reverted in
      commit df0b25b. Three-point A/B/C bisect confirmed the cache was the sole
      cause. Scratch buffer + prealloc retained as regression-free wins.
    commits:
      - "69ceca3 (added, tagged [SPINE-001] [SEARCH-CORE-001])"
      - "df0b25b (reverted, tagged [TAPE-001] only — should also carry [SPINE-001] per convention since it touches kernel/)"
evidence:
  bench_reports:
    - path: target/bench_reports/bench_report_v1_pre_tape_opt.json
      label: "A (pre-tape baseline)"
    - path: target/bench_reports/bench_report_v1_post_tape_opt.json
      label: "B (with ContentHash cache — regressed)"
    - path: target/bench_reports/bench_report_v1_scratch_prealloc_only.json
      label: "C (scratch+prealloc only — retained)"
  scale_1000_uniform_p50:
    A_pre_tape_ms: 73.6
    B_with_cache_ms: 87.9
    C_scratch_prealloc_ms: 73.6
  scale_1000_uniform_tape_p50:
    A_pre_tape_ms: 101.3
    C_scratch_prealloc_ms: 100.0
  conclusion: >-
    A→B shows +19.4% regression on search_fn_total (uniform, scale_1000 p50)
    caused solely by ContentHash struct bloat (~40→72 bytes). A→C shows no
    regression: scratch buffer + prealloc are regression-free wins. Tape
    overhead (C minus A on search_fn_total_with_tape minus search_fn_total)
    reduced from ~27.7ms to ~26.4ms (~5% improvement from scratch+prealloc).
  commits_shipped:
    - "3339f74 — scratch buffer (eliminates per-record Vec allocs)"
    - "3e61ed2 — pre-sized buffer from policy bounds"
    - "e8b1844 — hardening (lint allow on ScorerInputV1)"
    - "f507d69 — impossible-state fix + tape constant consolidation"
  commits_reverted:
    - "69ceca3 — ContentHash raw_sha256 cache (added)"
    - "df0b25b — ContentHash raw_sha256 cache (reverted)"
deferrals:
  - item: "hex decode elimination (~26ms at scale_1000)"
    reason: >-
      Not pursued until tape is always-on and proven worth the API surface
      change. The ~26ms overhead is dominated by hex::decode_to_slice in
      content_hash_to_raw(), called per-node and per-candidate during tape
      recording. Three non-bloat options documented for future consideration:
      (a) canonical_hash() returns (ContentHash, [u8;32]) tuple — callers
      that need raw keep it without inflating the type;
      (b) tape-local optimized 64-hex→32-byte decoder;
      (c) accept the decode cost as acceptable overhead.
    blocked_by: "decision on whether tape becomes always-on vs opt-in"
change_budget:
  max_files: 8
  max_loc: 300
